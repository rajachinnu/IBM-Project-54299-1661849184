{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset has been Downloaded and saved"
      ],
      "metadata": {
        "id": "V94sYPP2aeNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Image Augmentation"
      ],
      "metadata": {
        "id": "Rjnn-eA0awWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import load_img, img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator, image_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LvwlIp5-ay-W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Augmenting the Train Variables"
      ],
      "metadata": {
        "id": "crEPztHrbAU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 40,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    brightness_range = (0.5, 1.5)\n",
        ")"
      ],
      "metadata": {
        "id": "apEEgnyfbF9i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Augmenting the Test variables"
      ],
      "metadata": {
        "id": "OPuhc6JzbNdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale= 1./255)\n",
        "\n"
      ],
      "metadata": {
        "id": "AQGHHxY2bVa8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ftrain = train_datagen.flow_from_directory(\n",
        "    './flowers/train/',\n",
        "    target_size = (64,64),\n",
        "    class_mode = 'categorical',\n",
        "    batch_size = 100\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "FYzQOLaJbcgB",
        "outputId": "07402bdd-c93f-4951-8da6-faa4117cb87d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e81434a61647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclass_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m   def flow_from_dataframe(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m       \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m           \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './flowers/train/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 500 images belonging to 5 classes."
      ],
      "metadata": {
        "id": "eY7MZLHUbmV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ftest = test_datagen.flow_from_directory(\n",
        "    './flowers/test/',\n",
        "    target_size = (64,64),\n",
        "    class_mode = 'categorical',\n",
        "    batch_size = 100\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZsgtXO8QbneB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 500 images belonging to 5 classes.\n"
      ],
      "metadata": {
        "id": "UJ4dspLybyaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Creating the Model"
      ],
      "metadata": {
        "id": "zeGwWFASb4EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()"
      ],
      "metadata": {
        "id": "h3j4ziT0b-KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Adding Layers (Convolution Layers, MaxPooling, Flatten, Dense)"
      ],
      "metadata": {
        "id": "zXoiAY1ScHxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Convolution2D(32, (3,3), activation = 'relu', input_shape = (64, 64, 3)))\n",
        "model.add(MaxPooling2D(pool_size= (2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(400, activation = 'relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(200, activation = 'relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(5, activation = 'softmax'))"
      ],
      "metadata": {
        "id": "ej5UJgCqcNBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Compiling the Model"
      ],
      "metadata": {
        "id": "LTy5KefpcRhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "QxBnP9cxcX43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Fitting the model"
      ],
      "metadata": {
        "id": "Qomj8ozScg2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    ftrain,\n",
        "    steps_per_epoch = len(ftrain),\n",
        "    epochs = 10,\n",
        "    validation_data = ftest,\n",
        "    validation_steps = len(ftest)\n",
        ")"
      ],
      "metadata": {
        "id": "JNs8zSnbcntO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Saving the model"
      ],
      "metadata": {
        "id": "2cR9in20cxwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./flowers.h5')"
      ],
      "metadata": {
        "id": "iH1bCQdvc3OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Testing the model"
      ],
      "metadata": {
        "id": "v-uSQ_j4c69R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.1  Test 1"
      ],
      "metadata": {
        "id": "1INq73hadBEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = image_utils.load_img(\n",
        "    './flowers/test/daisy/1150395827_6f94a5c6e4_n.jpg',\n",
        "    target_size = (64,64)\n",
        ")\n",
        "f = image_utils.img_to_array(img)\n",
        "f = np.expand_dims(f, axis = 0)\n",
        "pred = np.argmax(model.predict(f))\n",
        "op = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
        "op[pred]"
      ],
      "metadata": {
        "id": "oXRlidJbdIVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.2 Test 2"
      ],
      "metadata": {
        "id": "L_02p3AYdPr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = image_utils.load_img(\n",
        "    './flowers/test/dandelion/7355522_b66e5d3078_m.jpg',\n",
        "    target_size = (64,64)\n",
        ")\n",
        "f = image_utils.img_to_array(img)\n",
        "f = np.expand_dims(f, axis = 0)\n",
        "pred = np.argmax(model.predict(f))\n",
        "op = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
        "op[pred]\n"
      ],
      "metadata": {
        "id": "DbXAwIIfdXW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: In both the above test cases the model has been passed"
      ],
      "metadata": {
        "id": "JrhA2Kvvdivh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "vhuxrSkwdvAw"
      }
    }
  ]
}